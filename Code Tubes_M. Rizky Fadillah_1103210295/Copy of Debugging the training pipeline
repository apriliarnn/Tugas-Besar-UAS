{"cells":[{"cell_type":"markdown","metadata":{"id":"aj3Ks2d0lOpQ"},"source":["# Debugging the training pipeline"]},{"cell_type":"markdown","metadata":{"id":"ePCXrbPUlOpR"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9or9gQq7lOpS"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"markdown","source":["Kode ini menginstal library utama untuk proyek NLP, yaitu datasets untuk memuat dan memproses dataset, evaluate untuk mengukur performa model, dan transformers[sentencepiece] untuk bekerja dengan model-model canggih dari Hugging Face yang menggunakan SentencePiece tokenizer, memungkinkan pengguna untuk melakukan berbagai tugas NLP secara efisien."],"metadata":{"id":"YriqDY1MlRf5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpJzctJ1lOpS","outputId":"7479ddba-3690-4ba4-cf42-4a589b88471d"},"outputs":[{"data":{"text/plain":["'ValueError: You have to specify either input_ids or inputs_embeds'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n",")\n","\n","raw_datasets = load_dataset(\"glue\", \"mnli\")\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n","\n","args = TrainingArguments(\n","    f\"distilbert-finetuned-mnli\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","metric = evaluate.load(\"glue\", \"mnli\")\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=raw_datasets[\"train\"],\n","    eval_dataset=raw_datasets[\"validation_matched\"],\n","    compute_metrics=compute_metrics,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-2WX0GFlOpT","outputId":"6a313060-eaef-49fb-abda-3b899bdcd184"},"outputs":[{"data":{"text/plain":["{'hypothesis': 'Product and geography are what make cream skimming work. ',\n"," 'idx': 0,\n"," 'label': 1,\n"," 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sRpcjsGlOpT","outputId":"4cb8f116-e92b-4137-f794-e509b078df3c"},"outputs":[{"data":{"text/plain":["'ValueError: expected sequence of length 43 at dim 1 (got 37)'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n",")\n","\n","raw_datasets = load_dataset(\"glue\", \"mnli\")\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n","\n","args = TrainingArguments(\n","    f\"distilbert-finetuned-mnli\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","metric = evaluate.load(\"glue\", \"mnli\")\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation_matched\"],\n","    compute_metrics=compute_metrics,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IK_ZVJ7lOpU","outputId":"92596445-9986-46a3-ead0-9d1dcf46121f"},"outputs":[{"data":{"text/plain":["'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdNLPweplOpU","outputId":"51697da7-312b-4b22-9bb6-a70cc8d62846"},"outputs":[{"data":{"text/plain":["dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train_dataset[0].keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gOGrKPplOpU","outputId":"083df2aa-4e40-4465-e127-2a0f76933c25"},"outputs":[{"data":{"text/plain":["transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["type(trainer.model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kqH5NtxlOpU","outputId":"b033e22d-bb61-4f7e-845f-708cafb19780"},"outputs":[{"data":{"text/plain":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train_dataset[0][\"attention_mask\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMo1L27ulOpU","outputId":"812db76a-4916-4dd9-a538-009315cc81be"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["len(trainer.train_dataset[0][\"attention_mask\"]) == len(\n","    trainer.train_dataset[0][\"input_ids\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cN1IoyGwlOpV","outputId":"04b24ac1-25e1-4e0b-8809-1415a5cea384"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train_dataset[0][\"label\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7epErSklOpV","outputId":"d4bcf4c7-4a42-4321-a9f6-f00d6feb8d1c"},"outputs":[{"data":{"text/plain":["['entailment', 'neutral', 'contradiction']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train_dataset.features[\"label\"].names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NlcKu7VlOpV","outputId":"b4d7c6fc-858b-41eb-e06a-d3e1ba3419ed"},"outputs":[{"data":{"text/plain":["~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)\n","    105                 batch[k] = torch.stack([f[k] for f in features])\n","    106             else:\n","--> 107                 batch[k] = torch.tensor([f[k] for f in features])\n","    108 \n","    109     return batch\n","\n","ValueError: expected sequence of length 45 at dim 1 (got 76)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["for batch in trainer.get_train_dataloader():\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plSukQCFlOpV","outputId":"b5c83546-6d8e-4919-f76b-fce0d24b6621"},"outputs":[{"data":{"text/plain":["<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["data_collator = trainer.get_train_dataloader().collate_fn\n","data_collator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNOwdEuZlOpV","outputId":"ac3bba29-2367-4bea-bdeb-54ef744b988c"},"outputs":[{"data":{"text/plain":["RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer,\n",")\n","\n","raw_datasets = load_dataset(\"glue\", \"mnli\")\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n","\n","args = TrainingArguments(\n","    f\"distilbert-finetuned-mnli\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","metric = evaluate.load(\"glue\", \"mnli\")\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation_matched\"],\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0efk7C49lOpW"},"outputs":[],"source":["data_collator = trainer.get_train_dataloader().collate_fn\n","batch = data_collator([trainer.train_dataset[i] for i in range(4)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBsHa0YplOpW"},"outputs":[],"source":["data_collator = trainer.get_train_dataloader().collate_fn\n","actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)\n","batch = data_collator([actual_train_set[i] for i in range(4)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSR4PdX4lOpW"},"outputs":[],"source":["for batch in trainer.get_train_dataloader():\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQn2UQXOlOpW","outputId":"a1d01a00-1de8-4e86-c1c0-fa6d6a4213f4"},"outputs":[{"data":{"text/plain":["~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n","   2386         )\n","   2387     if dim == 2:\n","-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n","   2389     elif dim == 4:\n","   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n","\n","IndexError: Target 2 is out of bounds."]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["outputs = trainer.model.cpu()(**batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouKUwrHslOpW","outputId":"6cef59cf-d5f3-48c2-cfcd-686cdf0f3fc4"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.model.config.num_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CunshcZKlOpW"},"outputs":[],"source":["from datasets import load_dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer,\n",")\n","\n","raw_datasets = load_dataset(\"glue\", \"mnli\")\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n","\n","args = TrainingArguments(\n","    f\"distilbert-finetuned-mnli\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","metric = evaluate.load(\"glue\", \"mnli\")\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation_matched\"],\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntJ2_cXulOpX"},"outputs":[],"source":["for batch in trainer.get_train_dataloader():\n","    break\n","\n","outputs = trainer.model.cpu()(**batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RS6255NylOpX"},"outputs":[],"source":["import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","batch = {k: v.to(device) for k, v in batch.items()}\n","\n","outputs = trainer.model.to(device)(**batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mj4qpDQylOpX"},"outputs":[],"source":["loss = outputs.loss\n","loss.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmLDBjvElOpX"},"outputs":[],"source":["trainer.create_optimizer()\n","trainer.optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hpIphZalOpX","outputId":"1cb14ee9-3639-40ec-a76f-f5ed7189cd63"},"outputs":[{"data":{"text/plain":["TypeError: only size-1 arrays can be converted to Python scalars"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# This will take a long time and error out, so you shouldn't run this cell\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onSCEqzilOpX","outputId":"4ad46ff9-77cd-495e-e708-708dad88f65c"},"outputs":[{"data":{"text/plain":["TypeError: only size-1 arrays can be converted to Python scalars"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydZcaS0XlOpX"},"outputs":[],"source":["for batch in trainer.get_eval_dataloader():\n","    break\n","\n","batch = {k: v.to(device) for k, v in batch.items()}\n","\n","with torch.no_grad():\n","    outputs = trainer.model(**batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Asfnzpp5lOpX","outputId":"0285bb87-fa65-4b97-9fb5-be7b1300cc5e"},"outputs":[{"data":{"text/plain":["TypeError: only size-1 arrays can be converted to Python scalars"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions = outputs.logits.cpu().numpy()\n","labels = batch[\"labels\"].cpu().numpy()\n","\n","compute_metrics((predictions, labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZwN0lWZlOpX","outputId":"a849bdfa-ebec-4792-ff10-c720fa3a80dd"},"outputs":[{"data":{"text/plain":["((8, 3), (8,))"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions.shape, labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysvC9YkXlOpY","outputId":"0703a351-64f6-4b73-a9b0-2889f85c4699"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.625}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","compute_metrics((predictions, labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAYgcrcglOpY"},"outputs":[],"source":["import numpy as np\n","from datasets import load_dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer,\n",")\n","\n","raw_datasets = load_dataset(\"glue\", \"mnli\")\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n","\n","args = TrainingArguments(\n","    f\"distilbert-finetuned-mnli\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","metric = evaluate.load(\"glue\", \"mnli\")\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation_matched\"],\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mgds3Y8llOpY"},"outputs":[],"source":["for batch in trainer.get_train_dataloader():\n","    break\n","\n","batch = {k: v.to(device) for k, v in batch.items()}\n","trainer.create_optimizer()\n","\n","for _ in range(20):\n","    outputs = trainer.model(**batch)\n","    loss = outputs.loss\n","    loss.backward()\n","    trainer.optimizer.step()\n","    trainer.optimizer.zero_grad()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AagoV82lOpY","outputId":"e1768200-dd17-4eca-90ca-b60a011c8a7a"},"outputs":[{"data":{"text/plain":["{'accuracy': 1.0}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["with torch.no_grad():\n","    outputs = trainer.model(**batch)\n","preds = outputs.logits\n","labels = batch[\"labels\"]\n","\n","compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))"]}],"metadata":{"colab":{"name":"Debugging the training pipeline","provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/en/chapter8/section4.ipynb","timestamp":1735998854591}]}},"nbformat":4,"nbformat_minor":0}